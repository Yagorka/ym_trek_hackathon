{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'all':[]}\n",
    "with open(\"/home/yagor/Рабочий стол/yandex generation/load data/pictures_market.txt\") as infile:\n",
    "    for line in infile:\n",
    "        result = json.loads(line)\n",
    "        d['all'].append(result)\n",
    "df = pd.DataFrame(d['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_photo_good_background     467031\n",
       "good_infographics               300772\n",
       "other_infographics              219703\n",
       "bad_infographics                 50719\n",
       "clean_photo_other_background     50496\n",
       "clean_photo_bad_background       28440\n",
       "clean_photo_image_background     14671\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info есть/нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infographic = {1: [ 'good_infographics', 'bad_infographics', 'other_infographics'], 0: ['clean_photo_image_background', 'clean_photo_good_background', 'clean_photo_bad_background']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = df.loc[df['verdict'].isin(infographic[1]+infographic[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENT_FRAC = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = info_df.groupby('verdict', as_index=False).apply(lambda x: x.sample(frac=PERCENT_FRAC, random_state=11)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[\"verdict\"] = np.where(sample_df[\"verdict\"].isin(infographic[0]), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bad/good info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "infographic = {1: [ 'good_infographics'], 0: ['bad_infographics']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = df.loc[df['verdict'].isin(infographic[1]+infographic[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_sku_id</th>\n",
       "      <th>pic_url</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>429050296</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/8786714...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>299703908</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1878151...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>186523793</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/9247093...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101872355836</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/7472696...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>101814268773</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/5415535...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131819</th>\n",
       "      <td>101781815789</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/8259349...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131821</th>\n",
       "      <td>101920408233</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/8604745...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131826</th>\n",
       "      <td>431075969</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/6018159...</td>\n",
       "      <td>bad_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131829</th>\n",
       "      <td>101815744136</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/7067214...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131830</th>\n",
       "      <td>101603206373</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1338103...</td>\n",
       "      <td>good_infographics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351491 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         market_sku_id                                            pic_url  \\\n",
       "4            429050296  //avatars.mds.yandex.net/get-marketpic/8786714...   \n",
       "6            299703908  //avatars.mds.yandex.net/get-marketpic/1878151...   \n",
       "8            186523793  //avatars.mds.yandex.net/get-marketpic/9247093...   \n",
       "10        101872355836  //avatars.mds.yandex.net/get-marketpic/7472696...   \n",
       "14        101814268773  //avatars.mds.yandex.net/get-marketpic/5415535...   \n",
       "...                ...                                                ...   \n",
       "1131819   101781815789  //avatars.mds.yandex.net/get-marketpic/8259349...   \n",
       "1131821   101920408233  //avatars.mds.yandex.net/get-marketpic/8604745...   \n",
       "1131826      431075969  //avatars.mds.yandex.net/get-marketpic/6018159...   \n",
       "1131829   101815744136  //avatars.mds.yandex.net/get-marketpic/7067214...   \n",
       "1131830   101603206373  //avatars.mds.yandex.net/get-marketpic/1338103...   \n",
       "\n",
       "                   verdict  \n",
       "4        good_infographics  \n",
       "6        good_infographics  \n",
       "8        good_infographics  \n",
       "10       good_infographics  \n",
       "14       good_infographics  \n",
       "...                    ...  \n",
       "1131819  good_infographics  \n",
       "1131821  good_infographics  \n",
       "1131826   bad_infographics  \n",
       "1131829  good_infographics  \n",
       "1131830  good_infographics  \n",
       "\n",
       "[351491 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {'bad_infographics':0, 'good_infographics': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good_infographics    300772\n",
       "bad_infographics      50719\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = info_df.replace({'verdict': di})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    300772\n",
       "0     50719\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_drop = info_df[info_df['verdict']==1].sample(frac=1).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 48179, 553694, 934342, 611737,  55915,  65486, 996666, 400951,\n",
       "            673298, 131684,\n",
       "            ...\n",
       "            501124, 378866, 746490, 838348, 754824,  44245,  85937, 331886,\n",
       "            276165, 330593],\n",
       "           dtype='int64', length=220772)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_index_drop = index_drop[80000:]\n",
    "real_index_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    80000\n",
       "0    50719\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = info_df.drop(real_index_drop)\n",
    "sample_df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bad/good/image back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {'clean_photo_good_background':0, 'clean_photo_image_background': 1, 'clean_photo_bad_background': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_photo_good_background', 'clean_photo_image_background', 'clean_photo_bad_background'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = df.loc[df['verdict'].isin(di.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = info_df.replace({'verdict': di})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_sku_id</th>\n",
       "      <th>pic_url</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1402035435</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/7044394...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1402035435</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/5620596...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>729239006</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1897710...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>429050296</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1857844...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>101858930853</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1776500...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131824</th>\n",
       "      <td>677173499</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/5115066...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131825</th>\n",
       "      <td>101121991197</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1662968...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131827</th>\n",
       "      <td>100818966350</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1697981...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131828</th>\n",
       "      <td>100972165930</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/1582458...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131831</th>\n",
       "      <td>101633095860</td>\n",
       "      <td>//avatars.mds.yandex.net/get-marketpic/8259349...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         market_sku_id                                            pic_url  \\\n",
       "0           1402035435  //avatars.mds.yandex.net/get-marketpic/7044394...   \n",
       "1           1402035435  //avatars.mds.yandex.net/get-marketpic/5620596...   \n",
       "3            729239006  //avatars.mds.yandex.net/get-marketpic/1897710...   \n",
       "5            429050296  //avatars.mds.yandex.net/get-marketpic/1857844...   \n",
       "12        101858930853  //avatars.mds.yandex.net/get-marketpic/1776500...   \n",
       "...                ...                                                ...   \n",
       "1131824      677173499  //avatars.mds.yandex.net/get-marketpic/5115066...   \n",
       "1131825   101121991197  //avatars.mds.yandex.net/get-marketpic/1662968...   \n",
       "1131827   100818966350  //avatars.mds.yandex.net/get-marketpic/1697981...   \n",
       "1131828   100972165930  //avatars.mds.yandex.net/get-marketpic/1582458...   \n",
       "1131831   101633095860  //avatars.mds.yandex.net/get-marketpic/8259349...   \n",
       "\n",
       "         verdict  \n",
       "0              0  \n",
       "1              0  \n",
       "3              0  \n",
       "5              0  \n",
       "12             0  \n",
       "...          ...  \n",
       "1131824        0  \n",
       "1131825        0  \n",
       "1131827        0  \n",
       "1131828        0  \n",
       "1131831        0  \n",
       "\n",
       "[510142 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    467031\n",
       "2     28440\n",
       "1     14671\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_drop = info_df[info_df['verdict']==0].sample(frac=1).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 922191,  210130,  695502,  663588,  894081,  221172,  300237,\n",
       "             213146, 1015472,  991262,\n",
       "            ...\n",
       "             271206,  117693,   36148,   74929, 1094829,  953533, 1008743,\n",
       "             458006, 1045584,  391561],\n",
       "           dtype='int64', length=417031)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_index_drop = index_drop[50000:]\n",
    "real_index_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50000\n",
       "2    28440\n",
       "1    14671\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = info_df.drop(real_index_drop)\n",
    "sample_df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  104575\n",
      "Validation size =  26144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(sample_df, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, df, mode = 'train', transform=None):\n",
    "        self.data = df\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.indexs = df.index.tolist()\n",
    "        self.bad_indexs = {}\n",
    "         \n",
    "    def __iter__(self):\n",
    "         # yield only valid data, skip Nones\n",
    "        for idx in self.indexs:\n",
    "            try:\n",
    "                if idx in self.bad_indexs:\n",
    "                    continue\n",
    "                else:\n",
    "                    url = self.data.loc[idx, 'pic_url']\n",
    "                    response = requests.get('https:'+ url)\n",
    "                    im = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "                    if self.mode=='train':\n",
    "                        label = self.data.loc[idx, 'verdict']\n",
    "                        label_tensor =  torch.as_tensor(label, dtype=torch.long)\n",
    "                \n",
    "                    if self.transform:\n",
    "                        im = self.transform(im)\n",
    "\n",
    "                    if self.mode == 'test':\n",
    "                        # For saving the predictions, the file name is required\n",
    "                        yield {'im' : im}\n",
    "                    else:\n",
    "                        yield {'im' : im, 'labels' : label_tensor}\n",
    "            except:\n",
    "                self.bad_indexs[url] = True\n",
    "        print(str(len(self.bad_indexs)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def shuffle(self):\n",
    "        # call this to shuffle the dirs between epochs\n",
    "        #self.dirs = [self.dirs[idx] for idx in torch.utils.data.RandomSample(range(len(self.indexs)))]\n",
    "        sample_df = info_df.groupby('verdict', as_index=False).apply(lambda x: x.sample(frac=PERCENT_FRAC, random_state=random.randint(0, 300))).reset_index(drop=True)\n",
    "        sample_df[\"verdict\"] = np.where(sample_df[\"verdict\"].isin(infographic[0]), 0, 1)\n",
    "        data_train, _ = train_test_split(sample_df, test_size=0.2, random_state=random.randint(0, 125))\n",
    "        self.data = data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means =  [0.485, 0.456, 0.406]\n",
    "stds  =  [0.229, 0.224, 0.225]\n",
    "\n",
    "size = 70\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms      =  transforms.Compose([\n",
    "                         #transforms.RandomRotation(30),\n",
    "                         transforms.Resize(size),\n",
    "                         transforms.CenterCrop(size),\n",
    "                         #transforms.RandomHorizontalFlip(p=0.5),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])\n",
    "\n",
    "test_valid_transforms =  transforms.Compose([\n",
    "                         transforms.Resize(size),\n",
    "                         transforms.CenterCrop(size),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(data_train, transform=train_transforms)\n",
    "valid_dataset = MyDataset(data_val, transform=test_valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "N_CORES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104575"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.data.loc[:, 'verdict'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyDataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_CORES)\n",
    "val_loader   =  torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=N_CORES)\n",
    "\n",
    "data_loaders =  {'train' : train_loader, 'valid': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model Dino v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "# Set seed\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yagor/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DinoVisionTransformerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DinoVisionTransformerClassifier, self).__init__()\n",
    "        self.transformer = dinov2_vits14\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model = DinoVisionTransformerClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 51/523 [09:21<1:05:44,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 60/523 [10:52<50:06,  6.49s/it]  /home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 100/523 [17:44<40:22,  5.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 151/523 [27:05<54:46,  8.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   150] loss: 0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 200/523 [35:30<29:46,  5.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 251/523 [44:18<35:49,  7.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   250] loss: 0.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 300/523 [52:35<23:03,  6.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 351/523 [1:01:43<22:05,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   350] loss: 0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 400/523 [1:09:59<12:34,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   400] loss: 0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 451/523 [1:19:22<10:14,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   450] loss: 0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 500/523 [1:27:44<02:05,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 522/523 [1:32:11<00:10, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    max_iter = len(train_dataset)//BATCH_SIZE\n",
    "    min_loss = np.inf\n",
    "    train_data_loader = tqdm.tqdm(data_loaders['train'])\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        if i>=max_iter:\n",
    "            break\n",
    "\n",
    "        inputs, labels = data['im'].to(device), data['labels'].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # torch.save(model, 'pretrain_model_transformer_info_not_info.pt')\n",
    "\n",
    "        if loss.item()<min_loss:\n",
    "            min_loss = loss.item()\n",
    "            torch.save(model, 'pretrain_model_transformer_good_bad_info_2class.pt')\n",
    "\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 38/131 [06:28<14:34,  9.41s/it] /home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/yagor/anaconda3/envs/detect/lib/python3.10/site-packages/PIL/Image.py:979: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 63%|██████▎   | 83/131 [14:22<07:06,  8.89s/it]"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# out = []\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    valid_data_loader = tqdm.tqdm(data_loaders['valid'])\n",
    "    max_iter_val = len(valid_dataset)//BATCH_SIZE\n",
    "    model.eval()\n",
    "\n",
    "    for i, data in enumerate(valid_data_loader):\n",
    "        if i>=max_iter_val:\n",
    "            break\n",
    "        images, labels = data['im'].to(device), data['labels']\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images.to(device))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        outmax, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.to(\"cpu\") == labels).sum().item()\n",
    "        # out.append(outmax.to(\"cpu\"))\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.27551020408163"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means =  [0.485, 0.456, 0.406]\n",
    "stds  =  [0.229, 0.224, 0.225]\n",
    "\n",
    "size = 70\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms      =  transforms.Compose([\n",
    "                         #transforms.RandomRotation(30),\n",
    "                         transforms.Resize(size),\n",
    "                         transforms.CenterCrop(size),\n",
    "                         #transforms.RandomHorizontalFlip(p=0.5),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])\n",
    "\n",
    "test_valid_transforms =  transforms.Compose([\n",
    "                         transforms.Resize(size),\n",
    "                         transforms.CenterCrop(size),\n",
    "                         transforms.ToTensor(),\n",
    "                         transforms.Normalize(means, stds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def create_model(MODEL_PATH, device=None):\n",
    "    model = torch.load(MODEL_PATH)\n",
    "    if not device:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "classes = ['good_infographics', 'bad_infographics', 'other_infographics', 'clean_photo_image_background', 'clean_photo_good_background', 'clean_photo_bad_background']\n",
    "model_info = create_model('/home/yagor/Рабочий стол/yandex generation/load data/pretrain_model_transformer_info_not_info_2claas.pt')\n",
    "model_info_bad_good = create_model('/home/yagor/Рабочий стол/yandex generation/load data/pretrain_model_transformer_good_bad_info_2class.pt')\n",
    "model_clean_image_bad_back = create_model('/home/yagor/Рабочий стол/yandex generation/load data/pretrain_model_transformer_good_bad_image_back.pt')\n",
    "models = {'Есть/нет инфографика': [model_info, {0:'Инфографики нет', 1:'Инфографика есть'}],\n",
    " 'Хорошая/плохая инфографика': [model_info_bad_good, {0:'bad_infographics', 1:'good_infographics'}],\n",
    " 'Какой фон': [model_clean_image_bad_back, {0:'clean_photo_good_background', 1:'clean_photo_image_background', 2:'clean_photo_bad_background'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(model, inp, device=None):\n",
    "    with torch.no_grad():\n",
    "        if not device:\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        outputs = model(inp.to(device))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.to('cpu')\n",
    "    return torch.nn.functional.softmax(outputs[0], dim=0), predicted\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def image_classification(inp, models_choice):\n",
    "    model, classes = models[models_choice]\n",
    "    inp = test_valid_transforms(inp).unsqueeze(0)\n",
    "    predictions, predicted = predict(model, inp)\n",
    "    confidences = {classes[i]: float(predictions[i]) for i in range(len(predictions))}\n",
    "    return confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(fn=image_classification,\n",
    "             inputs=[\n",
    "            gr.Image(type=\"pil\"), gr.Dropdown(['Есть/нет инфографика', 'Хорошая/плохая инфографика', 'Какой фон'], label=\"Выберите модель\", value='Есть/нет инфографика'), ],\n",
    "             outputs=gr.Label(num_top_classes=2),\n",
    "             examples=[[\"/home/yagor/Рабочий стол/yandex generation/load data/8794f117-98d8-494e-90d7-cbc2cedfa167.jpeg\"\n",
    "             ]]\n",
    "             ).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('detect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ac1247991103c5390c64217ddcfff4dc26afbd574cebcef9e0dc5c8418f9ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
